"""
CUBO Comprehensive Evaluation System
Stores detailed evaluation data and provides advanced analytics.
"""

import json
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
import logging
import os
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class QueryEvaluation:
    """Complete evaluation data for a single query."""
    # Required fields first (no defaults)
    timestamp: str
    session_id: str
    question: str
    answer: str
    response_time: float  # seconds
    contexts: List[str]
    context_metadata: List[Dict[str, Any]]  # filename, chunk_id, similarity_score, etc.
    model_used: str
    embedding_model: str
    retrieval_method: str
    chunking_method: str

    # Detailed Metrics (required)
    answer_length: int
    context_count: int
    total_context_length: int
    average_context_similarity: float
    answer_confidence: float  # if available

    # Quality Flags (required)
    has_answer: bool
    is_fallback_response: bool
    error_occurred: bool
    error_message: Optional[str]

    # Optional fields with defaults
    id: Optional[int] = None  # Primary key (auto-generated by database)
    answer_relevance_score: Optional[float] = None  # 0-1
    context_relevance_score: Optional[float] = None  # 0-1
    groundedness_score: Optional[float] = None  # 0-1

    # LLM-based Metrics
    llm_metrics: Optional[Dict[str, Any]] = None

    # User Feedback (future)
    user_rating: Optional[int] = None  # 1-5 stars
    user_feedback: Optional[str] = None

class EvaluationDatabase:
    """SQLite database for storing and analyzing evaluation data."""

    def __init__(self, db_path: str = "evaluation/evaluation.db"):
        """Initialize database connection."""
        self.db_path = db_path
        self._ensure_db_exists()
        self._create_tables()

    def _ensure_db_exists(self):
        """Ensure database directory exists."""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)

    def _create_tables(self):
        """Create database tables if they don't exist."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS evaluations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    session_id TEXT,
                    question TEXT NOT NULL,
                    answer TEXT,
                    response_time REAL,
                    contexts TEXT,  -- JSON array
                    context_metadata TEXT,  -- JSON array
                    model_used TEXT,
                    embedding_model TEXT,
                    retrieval_method TEXT,
                    chunking_method TEXT,

                    -- RAG Metrics
                    answer_relevance_score REAL,
                    context_relevance_score REAL,
                    groundedness_score REAL,

                    -- Detailed Metrics
                    answer_length INTEGER,
                    context_count INTEGER,
                    total_context_length INTEGER,
                    average_context_similarity REAL,
                    answer_confidence REAL,

                    -- Quality Flags
                    has_answer BOOLEAN,
                    is_fallback_response BOOLEAN,
                    error_occurred BOOLEAN,
                    error_message TEXT,

                    -- LLM Metrics
                    llm_metrics TEXT,

                    -- User Feedback
                    user_rating INTEGER,
                    user_feedback TEXT
                )
            ''')

            # Create indexes for performance
            conn.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON evaluations(timestamp)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_session_id ON evaluations(session_id)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_answer_relevance ON evaluations(answer_relevance_score)')

    def store_evaluation(self, evaluation: QueryEvaluation):
        """Store a query evaluation in the database."""
        data = asdict(evaluation)

        # Remove 'id' as it's AUTOINCREMENTED by the database
        if 'id' in data:
            del data['id']

        # Convert complex types to JSON
        data['contexts'] = json.dumps(data['contexts'])
        data['context_metadata'] = json.dumps(data['context_metadata'])
        data['llm_metrics'] = json.dumps(data['llm_metrics'])

        with sqlite3.connect(self.db_path) as conn:
            columns = ', '.join(data.keys())
            placeholders = ', '.join(['?' for _ in data])
            values = list(data.values())

            conn.execute(f'''
                INSERT INTO evaluations ({columns})
                VALUES ({placeholders})
            ''', values)

        logger.info(f"Stored evaluation for query: {evaluation.question[:50]}...")

    def get_recent_evaluations(self, limit: int = 100) -> List[QueryEvaluation]:
        """Get recent evaluations."""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            rows = conn.execute('''
                SELECT * FROM evaluations
                ORDER BY timestamp DESC
                LIMIT ?
            ''', (limit,)).fetchall()

        return [self._row_to_evaluation(row) for row in rows]

    def get_evaluations_by_date_range(self, start_date: str, end_date: str) -> List[QueryEvaluation]:
        """Get evaluations within date range."""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            rows = conn.execute('''
                SELECT * FROM evaluations
                WHERE timestamp BETWEEN ? AND ?
                ORDER BY timestamp DESC
            ''', (start_date, end_date)).fetchall()

        return [self._row_to_evaluation(row) for row in rows]

    def _row_to_evaluation(self, row) -> QueryEvaluation:
        """Convert database row to QueryEvaluation object."""
        data = dict(row)

        # Parse JSON fields
        data['contexts'] = json.loads(data['contexts'] or '[]')
        data['context_metadata'] = json.loads(data['context_metadata'] or '[]')
        data['llm_metrics'] = json.loads(data['llm_metrics'] or 'null')

        return QueryEvaluation(**data)

    def get_metrics_summary(self, days: int = 30) -> Dict[str, Any]:
        """Get comprehensive metrics summary."""
        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()

        with sqlite3.connect(self.db_path) as conn:
            # Basic counts
            total_queries, successful_queries = self._get_basic_counts(conn, cutoff_date)

            # Average scores
            average_scores = self._get_average_scores(conn, cutoff_date)

            # Score distributions
            score_distributions = self._get_score_distributions(conn, cutoff_date)

            # Top performing queries
            top_queries = self._get_top_performing_queries(conn, cutoff_date)

            # Error analysis
            common_errors = self._get_common_errors(conn, cutoff_date)

            return self._build_metrics_summary_response(
                days, total_queries, successful_queries,
                average_scores, score_distributions, top_queries, common_errors
            )

    def export_to_csv(self, output_path: str, days: int = 30):
        """Export evaluation data to CSV."""
        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()

        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query('''
                SELECT * FROM evaluations
                WHERE timestamp >= ?
                ORDER BY timestamp DESC
            ''', conn, params=(cutoff_date,))

        # Flatten JSON columns for CSV
        df['contexts'] = df['contexts'].apply(lambda x: json.loads(x) if x else [])
        df['context_metadata'] = df['context_metadata'].apply(lambda x: json.loads(x) if x else [])

        # Save to CSV (JSON columns will be stringified)
        df.to_csv(output_path, index=False)
        logger.info(f"Exported {len(df)} evaluations to {output_path}")

    def get_trends(self, days: int = 30) -> Dict[str, Any]:
        """Analyze performance trends over time."""
        cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()

        with sqlite3.connect(self.db_path) as conn:
            # Get daily statistics
            daily_stats = self._get_daily_statistics(conn, cutoff_date)

        if daily_stats.empty:
            return {'error': 'No data available for trend analysis'}

        # Calculate trends
        trends = self._calculate_trends(daily_stats)

        # Build response
        return self._build_trends_response(daily_stats, trends, days)

    def get_queries_needing_evaluation(self, session_id: Optional[str] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Get queries that haven't been evaluated yet (metrics are NULL or 0.0).

        Args:
            session_id: Only return queries from this session (optional)
            limit: Maximum number of queries to return (optional)

        Returns:
            List of dicts with query data
        """
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT id, question, answer, contexts, response_time, model_used, session_id
                FROM evaluations
                WHERE answer_relevance_score IS NULL OR answer_relevance_score = 0.0
            '''

            params = []
            if session_id:
                query += ' AND session_id = ?'
                params.append(session_id)

            query += ' ORDER BY timestamp DESC'

            if limit:
                query += ' LIMIT ?'
                params.append(limit)

            rows = conn.execute(query, params).fetchall()

            results = []
            for row in rows:
                results.append({
                    'id': row[0],
                    'question': row[1],
                    'answer': row[2],
                    'contexts': json.loads(row[3]) if row[3] else [],
                    'response_time': row[4],
                    'model_used': row[5],
                    'session_id': row[6]
                })

            return results

    def update_evaluation_metrics(self, evaluation_id: int, answer_relevance: float,
                                context_relevance: float, groundedness: float,
                                llm_metrics: Optional[Dict[str, Any]] = None):
        """
        Update evaluation metrics for an existing query.

        Args:
            evaluation_id: ID of the evaluation record to update
            answer_relevance: Answer relevance score (0-1)
            context_relevance: Context relevance score (0-1)
            groundedness: Groundedness score (0-1)
            llm_metrics: Additional LLM-based metrics (optional)
        """
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                UPDATE evaluations
                SET answer_relevance_score = ?,
                    context_relevance_score = ?,
                    groundedness_score = ?,
                    llm_metrics = ?
                WHERE id = ?
            ''', (
                answer_relevance,
                context_relevance,
                groundedness,
                json.dumps(llm_metrics) if llm_metrics else None,
                evaluation_id
            ))
            conn.commit()

    def _get_basic_counts(self, conn: sqlite3.Connection, cutoff_date: str) -> tuple[int, int]:
        """
        Get basic query counts (total and successful).

        Args:
            conn: Database connection
            cutoff_date: ISO format date string for filtering

        Returns:
            Tuple of (total_queries, successful_queries)
        """
        total_queries = conn.execute('''
            SELECT COUNT(*) FROM evaluations
            WHERE timestamp >= ?
        ''', (cutoff_date,)).fetchone()[0]

        successful_queries = conn.execute('''
            SELECT COUNT(*) FROM evaluations
            WHERE timestamp >= ? AND error_occurred = 0
        ''', (cutoff_date,)).fetchone()[0]

        return total_queries, successful_queries

    def _get_average_scores(self, conn: sqlite3.Connection, cutoff_date: str) -> tuple:
        """
        Get average scores for various metrics.

        Args:
            conn: Database connection
            cutoff_date: ISO format date string for filtering

        Returns:
            Tuple of average scores
        """
        scores = conn.execute('''
            SELECT
                AVG(answer_relevance_score) as avg_answer_relevance,
                AVG(context_relevance_score) as avg_context_relevance,
                AVG(groundedness_score) as avg_groundedness,
                AVG(response_time) as avg_response_time,
                AVG(answer_length) as avg_answer_length
            FROM evaluations
            WHERE timestamp >= ? AND error_occurred = 0
        ''', (cutoff_date,)).fetchone()

        return scores

    def _get_score_distributions(self, conn: sqlite3.Connection, cutoff_date: str) -> Dict[str, Dict[str, int]]:
        """
        Get score distributions for quality metrics.

        Args:
            conn: Database connection
            cutoff_date: ISO format date string for filtering

        Returns:
            Dictionary of score distributions by metric
        """
        score_distributions = {}
        for metric in ['answer_relevance_score', 'context_relevance_score', 'groundedness_score']:
            dist = conn.execute(f'''
                SELECT
                    COUNT(CASE WHEN {metric} >= 0.8 THEN 1 END) as excellent,
                    COUNT(CASE WHEN {metric} >= 0.6 AND {metric} < 0.8 THEN 1 END) as good,
                    COUNT(CASE WHEN {metric} >= 0.4 AND {metric} < 0.6 THEN 1 END) as fair,
                    COUNT(CASE WHEN {metric} < 0.4 THEN 1 END) as poor
                FROM evaluations
                WHERE timestamp >= ? AND error_occurred = 0 AND {metric} IS NOT NULL
            ''', (cutoff_date,)).fetchone()
            score_distributions[metric] = {
                'excellent': dist[0],
                'good': dist[1],
                'fair': dist[2],
                'poor': dist[3]
            }

        return score_distributions

    def _get_top_performing_queries(self, conn: sqlite3.Connection, cutoff_date: str) -> List[tuple]:
        """
        Get top performing queries by average score.

        Args:
            conn: Database connection
            cutoff_date: ISO format date string for filtering

        Returns:
            List of top query tuples
        """
        top_queries = conn.execute('''
            SELECT question, answer_relevance_score, context_relevance_score, groundedness_score
            FROM evaluations
            WHERE timestamp >= ? AND error_occurred = 0
            ORDER BY (answer_relevance_score + context_relevance_score + groundedness_score) / 3 DESC
            LIMIT 5
        ''', (cutoff_date,)).fetchall()

        return top_queries

    def _get_common_errors(self, conn: sqlite3.Connection, cutoff_date: str) -> List[tuple]:
        """
        Get most common error types.

        Args:
            conn: Database connection
            cutoff_date: ISO format date string for filtering

        Returns:
            List of error type tuples
        """
        error_types = conn.execute('''
            SELECT error_message, COUNT(*) as count
            FROM evaluations
            WHERE timestamp >= ? AND error_occurred = 1
            GROUP BY error_message
            ORDER BY count DESC
            LIMIT 5
        ''', (cutoff_date,)).fetchall()

        return error_types

    def _build_metrics_summary_response(self, days: int, total_queries: int, successful_queries: int,
                                      average_scores: tuple, score_distributions: Dict[str, Dict[str, int]],
                                      top_queries: List[tuple], common_errors: List[tuple]) -> Dict[str, Any]:
        """
        Build the final metrics summary response dictionary.

        Args:
            days: Number of days for the analysis period
            total_queries: Total number of queries
            successful_queries: Number of successful queries
            average_scores: Tuple of average scores
            score_distributions: Score distribution data
            top_queries: Top performing queries data
            common_errors: Common error data

        Returns:
            Complete metrics summary dictionary
        """
        return {
            'period_days': days,
            'total_queries': total_queries,
            'successful_queries': successful_queries,
            'success_rate': successful_queries / max(total_queries, 1),
            'average_scores': self._build_average_scores_section(average_scores),
            'score_distributions': score_distributions,
            'top_performing_queries': self._build_top_queries_section(top_queries),
            'common_errors': self._build_common_errors_section(common_errors),
            'generated_at': datetime.now().isoformat()
        }

    def _build_average_scores_section(self, average_scores: tuple) -> Dict[str, float]:
        """Build the average scores section of the response."""
        if not average_scores:
            return {}
            
        return {
            'avg_answer_relevance': average_scores[0],
            'avg_context_relevance': average_scores[1],
            'avg_groundedness': average_scores[2],
            'avg_response_time': average_scores[3]
        }

    def _build_top_queries_section(self, top_queries: List[tuple]) -> List[Dict[str, Any]]:
        """Build the top performing queries section of the response."""
        return [
            {
                'question': row[0],
                'answer_relevance_score': row[1],
                'context_relevance_score': row[2],
                'groundedness_score': row[3]
            } for row in top_queries
        ]

    def _build_common_errors_section(self, common_errors: List[tuple]) -> List[Dict[str, Any]]:
        """Build the common errors section of the response."""
        return [
            {
                'error_message': row[0],
                'count': row[1]
            } for row in common_errors
        ]