Immediate priority (MVP — get queryable system fast)

Fast-pass ingestion
Implement/verify FastPassIngestor: file scan, quick_text_extract, metadata parquet output.
Build BM25 index (Whoosh) and expose search API.
Add a simple CLI command and unit test for indexing a small folder.

Chunking + simple deep processing
Implement DeepIngestor for text/pdf/docx/csv chunking.
Save output as chunks parquet.
Write unit tests for chunk creation and chunk_id stability.

Dense embeddings + FAISS (local index)
Add EmbeddingGenerator (Sentence‑Transformers) and encode chunk/summaries.
Create FAISS HNSW index for hot set + small IVF on disk for cold set (configurable).
Provide a small script to load embeddings into FAISS and run sample lookups (unit/integration tests).

Basic retrieval + LLM generation pipeline
Implement hybrid retrieval step: BM25 → FAISS dense → merge top-K (naive fusion).
Hook to a local LLM (llama.cpp/Ollama) for single-shot prompt + response.
Add a simple CLI to query and return sources + answer.

Short-term enhancements (next iterations)
5. LLM processing for chunk enrichment

Summarization, keywords, categorization, self-consistency LLM pipeline.
Store scaffold summaries/paraphrases and embed them.
Unit test quality of summary/keywords on a few docs.

Semantic compression (scaffold)
Build logic to create compressed scaffolds and store mappings: scaffold→original chunks.
Implement retrieval fallback to original chunks when verifying grounding.

Deduplication (non-destructive)
Implement lightweight MinHash/SimHash prefilter and graph cluster dedup.
Keep canonical mapping (doc_uuid → canonical_uuid) and version metadata.

Hot/Cold indexing & atomic swaps
Implement hot index (HNSW in RAM) and cold (IVF+PQ on disk).
Implement append-only shards and atomic swap of live indices.
Unit tests for index load/unload and atomic swap.
