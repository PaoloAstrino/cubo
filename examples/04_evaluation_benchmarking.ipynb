{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# CUBO Evaluation & Benchmarking\n",
    "\n",
    "A professional RAG system isn't just about getting answersâ€”it's about measuring quality.\n",
    "This notebook demonstrates how to benchmark CUBO using Information Retrieval (IR) metrics and Generative Metrics (RAGAS).\n",
    "\n",
    "## What You'll Learn\n",
    "1. Create a synthetic Ground Truth dataset.\n",
    "2. Run retrieval evaluation (Recall@K).\n",
    "3. Run generative evaluation (RAGAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add CUBO to path\n",
    "cubo_root = Path(\".\").resolve().parent\n",
    "if str(cubo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(cubo_root))\n",
    "\n",
    "from cubo.core import CuboCore\n",
    "from evaluation.metrics import IRMetricsEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize core engine\n",
    "core = CuboCore()\n",
    "core.initialize_components()\n",
    "print(\"CUBO Core Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset",
   "metadata": {},
   "source": [
    "## 1. Create Ground Truth Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\"text\": \"The Eiffel Tower is located in Paris, France.\", \"id\": \"doc_paris\", \"file_path\": \"paris.txt\"},\n",
    "    {\"text\": \"The Colosseum is an ancient amphitheater in Rome, Italy.\", \"id\": \"doc_rome\", \"file_path\": \"rome.txt\"},\n",
    "    {\"text\": \"Sushi is a traditional dish from Japan.\", \"id\": \"doc_japan\", \"file_path\": \"japan.txt\"},\n",
    "    {\"text\": \"Pizza originated in Naples, Italy.\", \"id\": \"doc_naples\", \"file_path\": \"naples.txt\"}\n",
    "]\n",
    "\n",
    "core.add_documents(documents)\n",
    "\n",
    "ground_truth = {\n",
    "    \"Where is the Eiffel Tower?\": [\"doc_paris\"]},\n",
    "    \"Tell me about Italian food\": [\"doc_rome\", \"doc_naples\"]},\n",
    "    \"Famous buildings in Italy\": [\"doc_rome\"]},\n",
    "    \"Japanese cuisine\": [\"doc_japan\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval-eval",
   "metadata": {},
   "source": [
    "## 2. Evaluate Retrieval (Recall@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for query, relevant_ids in ground_truth.items():\n",
    "    retrieved = core.query_retrieve(query, top_k=3)\n",
    "    retrieved_ids = [doc['metadata'].get('id') for doc in retrieved]\n",
    "    \n",
    "    metrics = IRMetricsEvaluator.evaluate_retrieval(\n",
    "        question_id=query,\n",
    "        retrieved_ids=retrieved_ids,\n",
    "        ground_truth={query: relevant_ids},\n",
    "        k_values=[1, 3]\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"recall@3\": metrics[\"recall_at_k\"][3],\n",
    "        \"mrr\": metrics[\"mrr\"]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "print(f\"Avg Recall@3: {df['recall@3'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ragas-eval",
   "metadata": {},
   "source": [
    "## 3. Evaluate Generation (RAGAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-ragas",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from evaluation.ragas_evaluator import run_ragas_evaluation\n",
    "    \n",
    "    questions = list(ground_truth.keys())\n",
    "    ground_truths_list = [\"Paris\", \"Italian food\", \"Colosseum\", \"Sushi\"]\n",
    "    \n",
    "    contexts = []\n",
    "    answers = []\n",
    "    \n",
    "    for q in questions:\n",
    "        res = core.query_and_generate(q, top_k=2)\n",
    "        answers.append(res['answer'])\n",
    "        contexts.append([d['document'] for d in res['sources']])\n",
    "\n",
    "    scores = run_ragas_evaluation(questions, contexts, ground_truths_list, answers)\n",
    "    print(\"RAGAS Scores:\", scores)\n",
    "except Exception as e:\n",
    "    print(\"RAGAS Eval skipped:\", str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}