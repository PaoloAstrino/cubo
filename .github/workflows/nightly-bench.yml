name: Nightly Benchmarks

on:
  schedule:
    # Run every night at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  benchmark-small-datasets:
    name: BEIR Small Datasets Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 240  # 4 hours max
    
    strategy:
      matrix:
        dataset: [nfcorpus, scifact, arguana, fiqa]
      fail-fast: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install faiss-cpu
      
      - name: Download BEIR dataset
        run: |
          mkdir -p data/beir/${{ matrix.dataset }}
          # Dataset download would happen here
          echo "Dataset download for ${{ matrix.dataset }}"
      
      - name: Run BEIR adapter (with reindex)
        run: |
          python -u scripts/run_beir_adapter.py \
            --corpus data/beir/${{ matrix.dataset }}/corpus.jsonl \
            --queries data/beir/${{ matrix.dataset }}/queries.jsonl \
            --output results/beir_run_${{ matrix.dataset }}.json \
            --index-dir results/beir_index_${{ matrix.dataset }} \
            --reindex \
            --laptop-mode
      
      - name: Calculate metrics
        run: |
          python scripts/calculate_beir_metrics.py \
            --results results/beir_run_${{ matrix.dataset }}.json \
            --qrels data/beir/${{ matrix.dataset }}/qrels/test.tsv \
            --k 10
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.dataset }}
          path: |
            results/beir_run_${{ matrix.dataset }}.json
            results/beir_run_${{ matrix.dataset }}_metrics_k10.json
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-logs-${{ matrix.dataset }}
          path: results/logs/

  rrf-parameter-sweep:
    name: RRF Parameter Sweep
    runs-on: ubuntu-latest
    needs: benchmark-small-datasets
    timeout-minutes: 120
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: results/
      
      - name: Run RRF parameter sweep
        run: |
          python -u scripts/rrf_sensitivity_sweep.py \
            --datasets nfcorpus scifact arguana fiqa
      
      - name: Compare RRF configurations
        run: |
          python scripts/compare_rrf_best.py
      
      - name: Upload RRF results
        uses: actions/upload-artifact@v4
        with:
          name: rrf-sweep-results
          path: |
            results/*_rrf_*_metrics_k10.json

  regression-check:
    name: Regression Check
    runs-on: ubuntu-latest
    needs: rrf-parameter-sweep
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-ci.txt
      
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: results/
      
      - name: Run regression tests
        run: |
          pytest tests/regression/ -v --tb=short
      
      - name: Check for performance degradation
        run: |
          python scripts/check_metrics_regression.py || echo "Check complete"
      
      - name: Upload regression report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: regression-report
          path: |
            regression_report.json

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: benchmark-small-datasets
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-ci.txt
          pip install faiss-cpu
      
      - name: Run E2E tests
        run: |
          pytest tests/e2e/ -v --tb=short -m "not slow"
      
      - name: Upload E2E test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: |
            .pytest_cache/
            test-results.xml

  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [benchmark-small-datasets, rrf-parameter-sweep, regression-check, e2e-tests]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Generate summary report
        run: |
          echo "# Nightly Benchmark Summary" > summary.md
          echo "" >> summary.md
          echo "## Benchmark Results" >> summary.md
          echo "- nfcorpus: ${{ needs.benchmark-small-datasets.result }}" >> summary.md
          echo "- scifact: ${{ needs.benchmark-small-datasets.result }}" >> summary.md
          echo "- arguana: ${{ needs.benchmark-small-datasets.result }}" >> summary.md
          echo "- fiqa: ${{ needs.benchmark-small-datasets.result }}" >> summary.md
          echo "" >> summary.md
          echo "## RRF Parameter Sweep" >> summary.md
          echo "Status: ${{ needs.rrf-parameter-sweep.result }}" >> summary.md
          echo "" >> summary.md
          echo "## Regression Check" >> summary.md
          echo "Status: ${{ needs.regression-check.result }}" >> summary.md
          echo "" >> summary.md
          echo "## E2E Tests" >> summary.md
          echo "Status: ${{ needs.e2e-tests.result }}" >> summary.md
      
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary
          path: summary.md
      
      - name: Comment on latest commit (if failure)
        if: |
          needs.benchmark-small-datasets.result == 'failure' ||
          needs.regression-check.result == 'failure'
        run: |
          echo "Benchmark or regression check failed!"
          echo "Check artifacts for details"

  notify-slack:
    name: Notify Slack
    runs-on: ubuntu-latest
    needs: benchmark-summary
    if: always() && github.event_name == 'schedule'
    
    steps:
      - name: Send Slack notification
        if: env.SLACK_WEBHOOK_URL != ''
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          curl -X POST $SLACK_WEBHOOK_URL \
            -H 'Content-Type: application/json' \
            -d '{
              "text": "Nightly benchmark completed",
              "attachments": [{
                "color": "${{ needs.benchmark-summary.result == \"success\" && \"good\" || \"danger\" }}",
                "text": "Status: ${{ needs.benchmark-summary.result }}"
              }]
            }' || echo "Slack notification skipped"
